# ğŸŒŸ MWP: MLLM-Guided Weak Prior for Cross-Modal Retrieval

<p align="center">
  <img src="assets/banner.png" width="92%" alt="MWP Banner"/>
</p>

<p align="center">
  <a href="#-overview">Overview</a> â€¢
  <a href="#-datasets">Datasets</a> â€¢
  <a href="#-installation">Installation</a> â€¢
  <a href="#-data-preparation">Data Preparation</a> â€¢
  <a href="#-training">Training</a> â€¢
  <a href="#-evaluation">Evaluation</a> â€¢
  <a href="#-results">Results</a>
</p>

---

## âœ¨ Overview

**MWP** is a weak-prior guided cross-modal retrieval framework that leverages *noisy MLLM-generated image descriptions* to build a **weak structural prior (WPE)** and then trains a **deployable retriever (GMR)** under this guidance.

ğŸ”‘ Key idea:
- ğŸŒ¤ï¸ **WPE (Weak Prior Estimator)** learns / produces a **soft prior similarity** \( S^{T} \) from *(description, caption)* pairs.  
- ğŸ” **GMR (Granular Multi-Retriever)** learns **retrieval logits** \( S^{S} \) from *(caption, image)* pairs and aligns with WPE priors.

ğŸ“Œ Training is **two-stage**:
1. **Stage-1:** Train **WPE** (prior modeling)  
2. **Stage-2:** Train **GMR** guided by frozen WPE (prior alignment)

---

## ğŸ“š Datasets

We evaluate MWP on four widely-used cross-modal retrieval datasets:

- **Wikipedia**
- **Pascal Sentence**
- **NUS-WIDE-10k**
- **XMediaNet**

> ğŸ§© Notes  
> - Each sample contains: image, caption/text, and a category label (for category-aware training/evaluation).  
> - We additionally use **MLLM-generated image descriptions** (noisy) to train or infer WPE priors.

---

## ğŸ§° Installation

### âœ… Requirements
- Python >= 3.9
- PyTorch >= 1.13 (recommended 2.x)
- transformers
- tqdm, numpy, pandas

### âš™ï¸ Setup

```bash
conda create -n mwp python=3.10 -y
conda activate mwp

pip install -r requirements.txt
